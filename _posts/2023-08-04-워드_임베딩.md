---
title: 워드 임베딩 - Word2Vec 이해하기
author: HyunMin Kim
date: 2023-08-04 00:00:00 0000
categories: [Data Science, NLP]
tags: [Word2Vec]
---

## 1. 워드 임베딩이란?
텍스트 데이터를 자연어 처리나 기계 학습 알고리즘에 사용할 수 있는 형태로 변환하는 기법 중 하나임. 워드 임베딩의 핵심 아이디어는 **단어의 의미는 주변 단어에 의해 결정된다**는 분포 가설(distributional hypothesis)에 기반한다.

워드 임베딩을 통해, 각 단어는 고차원 공간에 벡터로 표현된다. 벡터 공간에서는 의미상 유사한 단어들이 서로 가깝게 위치하게 되며 이러한 벡터 표현은 단어 간의 유사도를 계산하거나, 단어 사이의 관계를 파악하는 데 사용될 수 있다.

![258587729-0f3014c5-2aee-4723-8eb2-163e2565a865 (1)](https://github.com/hmkim312/project/assets/60168331/1948e357-2191-4331-901c-338d9c24f18b)



## 2. Word2Vec
Word2Vec은 구글이 개발한 단어 임베딩 기법으로 주변 단어의 유사성을 이용하여 각 단어에 대한 벡터를 학습한다. Word2Vec에는 **Continuous Bag of Words(CBOW)** 모델과 **Skip-Gram** 모델이 있다.


**CBOW** 모델은 주변 단어들 W(t-2,-1,+1,+2)을 기반으로 중심 단 어W(t)를 예측한다. 예를 들어, "The cat sits on the mat"라는 문장이 있을 때 "The", "cat", "on", "the", "mat"을 주변 단어로 사용하여 "sits"를 예측하는 것을 목표로 한다. CBOW 모델은 Skip-Gram 모델에 비해 빠르게 학습되며, 빈도수가 많은 단어에 대해 잘 작동하는 특징이 있다.

**Skip-Gram** 모델은 중심 단어를 기반으로 주변 단어를 예측한다. 위의 예제에서는 "sits"가 중심 단어라면 "The", "cat", "on", "the", "mat"을 예측하는 것을 목표로 한다. Skip-Gram 모델은 희소한 데이터 집합에서 잘 작동하며, 단어의 유사성을 잘 잡아내는 특장이 있다.

![Untitled](https://github.com/hmkim312/project/assets/60168331/09e8b1f9-78c9-4bdb-b810-08268ff26739)

이렇게 학습된 Word2Vec 모델은 단어 간의 **유사도**를 계산하는데 사용될 수 있으며, 이를 바탕으로 텍스트를 이해하거나 텍스트 분류, 감성 분석, 기계 번역 등 다양한 자연어 처리 작업을 수행하는데 기본이 되는 개념이다.


## 3. 실습: Word2Vec 임베딩 사용하기
```python
import gensim.downloader as api

# Word2vec-google-news-300 임베딩 모델 다운
model = api.load('word2vec-google-news-300')
```
Python의 Gensim 라이브러리를 사용하면 사전 학습된 Word2Vec 임베딩을 손쉽게 사용할 수 있다. 아래 코드는 Gensim을 사용하여 Word2Vec-google-news-300 임베딩 모델을 인터넷에서 다운로드하고, 특정 단어에 대한 임베딩을 벡터를 가져오는 코드이다. 
> "word2vec-google-news-300"은 Word2Vec을 사용하여 구축된 대규모 워드 임베딩 모델로 300차원의 벡터 공간에 약 3백만 개의 영어 단어와 구문을 포함한 약 100억개의 단어 토큰을 가지고 있는 대규모 영어 단어 임베딩 모델이다. 구글 뉴스 기사와 같은 방대한 양의 텍스트 데이터를 바탕으로 학습되었다.

### 3.1 임베딩 백터
```python
# king의 임베딩 벡터 
embedding = model['king']

print(embedding)

# output
[ 1.25976562e-01, 2.97851562e-02,  8.60595703e-03, 1.39648438e-01,
 -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,
  5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,
 -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,
  3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,
  1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,
  1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02, ... ]
```

gensim에서 임베딩 모델을 불러와 모델로 지정하고, 그 모델에서 **king**의 임베딩 벡터를 쉽게 가져올수 있다. 결과로 나온 숫자는 다른 단어들과의 관계를 고려해 생성된 숫자로 우리가 어떤 의미를 가지고 있는지 직관적으로 알 수는 없다. 다만, 임베딩 벡터가 유사도를 계산하는 등 여러가지 작업의 기본이 된다.

### 3.2 유사도 (similarity) 계산 

```python
# Compute similarity between "king" and "queen"
similarity = model.similarity('king', 'queen')
print(f'Similarity between "king" and "queen": {similarity}')

# Compute similarity between "king" and "apple"
similarity = model.similarity('king', 'apple')
print(f'Similarity between "king" and "apple": {similarity}')

# output
Similarity between "king" and "queen": 0.6510956883430481
Similarity between "king" and "apple": 0.10826095938682556
```



**유사도 계산**이란 두 단어간 유사한 정도를 숫자로 표현하는 것이며 유사도를 `-1 ~ 1` 혹은 `0 ~ 1` 사이의 값을 가지며 숫자가 낮을수록 유사하지 않음을 나타낸다.
gensim의 임베딩 모델에서 두 단어 간의 유사도 similarity 함수를 사용하여 계산한다. 위의 예제의 **king**과 **queen**의 유사도는 0.65 수준으로 나타났으며 **king**과 **apple**은 거의 유사하지 않아 0.1 수준으로 나타났다.
유사도 계산을 통해 입력된 단어와 비슷한 단어를 추천 받을 수도 있으며, 비슷한 정도를 통해 동일 카테고리로 설정하는 듯 여러가지 작업이 가능하다.

### 3.3 단어 유추 (Word analogy)

```python
# 남자는 왕일때 여자는? =  king + woman - man ?
similar_word = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
print(f'남자는 왕일때 여자는? {similar_word[0][0]}')

# output
남자는 왕일때 여자는? queen
```

**단어 유추**는 단어 간의 유사성을 통해 새로운 단어를 찾아내는 데 사용되는 문제 유형입니다. 이 유형의 문제는 보통 "A는 B일때 C는 무엇인가요?" 같은 형태로 표현된다. 단어 유추의 간단한 실습은 <a href="https://word2vec.kr/search/" target="_blank">Word2vec</a>에서도 가능하다.

## 4 마치며
워드 임베딩은 자연어 처리의 핵심적인 요소 중 하나로 Word2Vec 같은 기법은 텍스트 데이터를 다루는 다양한 작업에서 높은 성능을 보여주며, 이는 현재도 많은 연구에서 사용되고 있다. 이 게시물이 워드 임베딩에 대한 이해에 도움이 되기를 바란다.