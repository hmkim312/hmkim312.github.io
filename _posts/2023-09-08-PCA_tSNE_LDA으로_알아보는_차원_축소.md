---
title: PCA, t-SNE, LDA으로 알아보는 차원 축소
author: HyunMin Kim
date: 2023-09-08 00:00:00 0000
categories: [Data Science, Machine Learning]
tags: [PCA, t-SNE, LDA]
image: https://github.com/hmkim312/datas/assets/60168331/c0589161-1276-4546-9a03-6788ca0b1618
---

차원 축소는 데이터의 차원을 줄여서 데이터를 간결하게 만드는 기술이다. 시각화, 데이터 축소, 노이즈 제거, 성능 향상 및 계산 시간 감소를 위해 사용한다. 대표적인 차원 축소 알고리즘으로는 PCA, t-SNE, LDA가 있으며, 각 알고리즘은 장점과 단점이 있다. 차원 축소는 머신 러닝 모델의 성능에 긍부적 적인 영향을 미치며, 특성 선택과 차원 축소는 정보 유지와 계산 효율성 측면에서 차이가 있다.

## 1. 서론
---
### 1) 차원의 저주 (Curse of Dimensionality)
보통 우리가 생각하는 데이터는 여러 가지 정보로 이루어져 있다. 온라인 쇼핑 사이트에서 상품을 구매할 때를 예를 들자면 상품의 가격, 색상, 브랜드, 평점 및 후기 등 여러 정보가 있다. 이러한 정보를 **차원**이라고 한다. 하지만 데이터의 차원(정보)이 많아질수록 데이터가 복잡해져 분석하거나 활용하는 것이 어려워진다. 이런 문제를 **차원의 저주**라고 한다.
### 2) 차원 축소(Demension Reduce)의 중요성 및 응용 분야
차원의 저주와 같은 문제를 해결하기 위해서는 데이터의 차원, 즉 정보의 양을 효과적으로 줄이는 방법이 필요하다. 중요한 것은 정보의 양을 줄이면서도 데이터의 핵심 특징을 잃지 않는 것이다. 이렇게 데이터를 간결하게 만드는 과정을 **차원 축소**라고 한다.

차원 축소는 다양한 분야에서 사용된다. 예를 들면, 복잡한 데이터를 간단한 그래프로 시각화하거나 대량의 데이터를 빠르게 처리해야 할 때 차원 축소 기법이 활용된다. 이 글에서는 차원 축소의 기본 개념과 주요한 세 가지 방법을 소개하려 한다.

## 2. 기본 개념
---
### 1) 차원이란 무엇인가?
차원을 간단히 정의하면 어떤 정보나 특성을 나타내는 축이다. 예를 들어, 평면 위의 점을 나타낼 때 가로 축과 세로 축, 두 가지 정보가 필요하다. 이렇게 두 가지 정보를 나타내는 축을 두 개의 차원이라고 한다. 공간에서는 높이라는 세 번째 축이 추가되어 총 세 개의 차원으로 된다. 또한, 시간의 개념이 추가되면 4차원이되며 더 많은 차원이 추가되어 4차원 이상이되면 그때부터는 사람의 머리로는 상상하기 어려워 진다.
### 2) 데이터의 차원과 특성
데이터에서의 차원도 비슷한 개념이다. 데이터가 여러 가지 특성을 가지고 있을 때, 각 특성은 하나의 차원으로 생각할 수 있다. 예를 들어, 자동차와 관련된 데이터가 있다고 하면 자동차의 색깔, 크기, 브랜드, 연비 등 다양한 특성이 있을 것이다. 여기서 이야기하는 특성이 데이터의 차원이라고 할 수 있다. 따라서 데이터의 차원은 그 데이터가 얼마나 많은 **특성**을 가지고 있는지를 나타낸다.

## 3. 주성분 분석 (PCA, Principal Component Analysis)
---
### 1) 기본 원리 및 수학적 배경
**주성분 분석**은 데이터의 주요 특성을 찾아내는 방법이다. 데이터에 내재된 패턴이나 구조를 찾아서 데이터를 더 간단한 형태로 표현하려는 것이 목표다.

수학적으로 보면, PCA는 데이터의 분산이 최대가 되는 방향을 찾는다. 이 방향은 **주성분**이라고 불리며, 데이터의 핵심 정보를 가장 잘 나타낸다. 처음 찾은 주성분과 직교하는 방향 중에서 다음으로 분산이 큰 방향을 두 번째 주성분으로, 그 다음 방향을 세 번째 주성분으로 정하며 이런식으로 설정한 수만큼 주성분을 찾게 된다.
### 2) 장점 및 한계
PCA의 큰 장점 중 하나는 복잡한 데이터를 몇 개의 주성분만으로 간단하게 표현할 수 있다는 점이다. 이로 인해 데이터의 시각화나 분석이 훨씬 간단해 진다.
그러나 PCA에도 한계가 있다. PCA는 선형 관계만을 판단하므로, 데이터가 비선형 구조를 가질 때는 그 특성을 제대로 파악하기 어렵다. 또한, 모든 주성분이 데이터의 중요한 특성을 반드시 반영한다는 보장이 없다. 따라서 PCA 결과를 해석할 때는 주의가 필요하다.

### 3) 실습
> iris 데이터셋을 사용하여 실습한다. 이 데이터셋에는  3개의 꽃 종류(setosa, versicolor, virgnica)에 대한 4개의 특성(꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비)이 포함되어 있다.

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# 데이터셋 로드
iris = datasets.load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names

# 시각화 설정
sns.set_style("whitegrid")

# 주성분을 2개로 한 PCA 객체 생성
pca = PCA(n_components=2)
# 데이터를 PCA로 변환
X_pca = pca.fit_transform(X)

plt.figure(figsize=(8, 5))
colors = ['navy', 'turquoise', 'darkorange']
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=2,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('PCA')
plt.show()

```
<img src ="https://github.com/hmkim312/datas/assets/60168331/c0589161-1276-4546-9a03-6788ca0b1618" alt="PCA" style="max-width: 100%; height: auto;">

4개의 특성을 2개로 PCA 한 결과이다. iris 3개의 종 중 setosa와 나머지 2개는 잘 구분하겠으나, versicolor와 virginica는 구분선이 모호한것으로 보인다.

## 4. t-SNE (t-Distributed Stochastic Neighbor Embedding)
---
### 1) 기본 원리 및 수학적 배경
t-SNE는 고차원 데이터를 저차원(주로 2차원이나 3차원)으로 시각화하기 위한 기법 중 하나다. 원리는 간단하게 말하자면, 고차원에서의 데이터 포인트 간의 유사성과 저차원에서의 유사성이 최대한 비슷하도록 데이터를 변환한다.

고차원에서의 유사성은 가우시안 분포를 사용하여 측정하고, 저차원에서는 t-분포를 사용한다. 이렇게 두 분포를 사용하는 이유는 저차원에서 데이터 포인트들이 서로 너무 뭉치지 않게 하기 위해서이다.
### 2) 장점 및 한계
t-SNE의 가장 큰 장점은 복잡한 데이터 구조 (e.g 비선형)를 잘 표현한다는 것이다. 그로 인해 많은 데이터 시각화에서 t-SNE를 활용하여 데이터의 구조를 이해하기 쉽게 만든다.

그렇지만 t-SNE에도 몇 가지 한계가 있다. 첫째, 학습률이나 초기 설정 값에 따라 결과가 변한다. 이것은 여러 번 시도하여 최적의 결과를 얻어야 함을 의미한다. 둘째, 저차원으로 변환된 결과가 원래의 고차원 데이터와 얼마나 유사한지를 정량적으로 평가하기 어렵다. 마지막으로, 대규모 데이터에 t-SNE를 적용하려면 계산 시간이 많이 필요하다.

### 3) 실습
```python
# 주성분을 2개로 한 t-SNE 객체 생성
tsne = TSNE(n_components=2, random_state=0)
# 데이터를 t-SNE 변환
X_tsne = tsne.fit_transform(X)

plt.figure(figsize=(8, 5))
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_tsne[y == i, 0], X_tsne[y == i, 1], color=color, alpha=.8, lw=2,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('t-SNE')
plt.show()
```

<img src ="https://github.com/hmkim312/datas/assets/60168331/52cca270-b58d-4c42-b24d-b92103afc63a" alt="t-SNE" style="max-width: 100%; height: auto;">

4개의 특성을 2개로 t-SNE 한 결과이다. 결과는 PCA와 마찬가지로 iris 3개의 종 중 setosa와 나머지 2개는 잘 구분하겠으나, versicolor와 virginica는 구분선이 모호한것으로 보인다.

## 5. 선형 판별 분석 (LDA, Linear Discriminant Analysis)
---
### 1) 기본 원리 및 수학적 배경
선형 판별 분석 (LDA)은 주로 분류 문제에서 사용되는 방법으로, 데이터의 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하여 데이터를 선형적으로 분리하는 방법이다.

수학적으로, LDA는 클래스 간의 분산과 클래스 내의 분산의 비율을 최대화하는 방향을 찾는다. 이러한 방향은 데이터가 속한 클래스를 가장 잘 구분할 수 있는 방향을 찾는것이다. 따라서 방향을 찾는것이 LDA의 주요 목적이다. LDA는 주로 두 개의 클래스를 가진 문제에서 사용하지만 3개 이상인 다중 클래스 문제에도 사용 할 수 있다.

### 2) 장점 및 한계
LDA의 주된 장점은 클래스 간 분리를 최적화하여 분류 문제에서 뛰어난 성능을 보인다는 점이다. 또한, 차원 축소 기법으로 사용될 때, 클래스 정보를 유지하면서 데이터를 저차원으로 표현하는 능력이 있다.

그러나 LDA의 한계도 존재한다. 데이터의 클래스 분포가 균일하지 않다면 LDA는 제대로 동잗하지 않을 수 있다. 또한, LDA는 선형적인 클래스의 구분만을 고려하므로, 데이터의 클래스 경계가 비선형일 경우 클래스간 구분선을 제대로 파악하기 어려울 수 있다. 마지막으로, LDA는 모든 클래스에 공통적인 공분산을 가정하는데, 이 가정이 항상 맞는것은 아니다. 따라서 LDA를 사용할 때 이러한 한계점들을 고려하고 사용하는 것이 중요하다.

### 3) 실습
```python
lda = LDA(n_components=2)
# 데이터를 LDA로 변환. LDA는 지도 학습이므로 타겟 레이블도 함께 제공.
X_lda = lda.fit_transform(X, y)

plt.figure(figsize=(8, 5))

for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], alpha=.8, color=color,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('LDA')
plt.show()
```

<img src ="https://github.com/hmkim312/datas/assets/60168331/3b1a6f14-df9c-4809-8a1a-35f6915437f5" alt="LDA" style="max-width: 100%; height: auto;">

4개의 특성을 2개로 LDA 한 결과이다. 결과는 PCA, t-SNE 마찬가지로 iris 3개의 종 중 setosa와 나머지 2개는 잘 구분하겠으나, versicolor와 virginica는 구분선이 선형이 아니라 모호한것으로 보인다.

## 6. 차원 축소의 응용 분야
---
### 1) 시각화
차원 축소는 고차원 데이터를 2차원이나나 3차원으로 변환하여 시각적으로 표현할 수 있게 해준다. 이를 통해 데이터의 구조나 패턴을 직관적으로 이해하게 되며, 숨겨진 그룹이나 이상치 같은 특징을 쉽게 파악할 수 있다.

### 2) 데이터 압축
차원 축소는 데이터의 크기를 줄여준다. 이것은 메모리나 하드디스크, 데이터 베이스에 저장 공간을 절약하는데 도움이 된다. 더욱이, 압축된 데이터는 원래 데이터의 중요한 특성을 유지하면서 크기가 줄어들기 때문에, 계산 비용이나 저장 비용을 절약할 수 있다.

### 3) 노이즈 제거
데이터에는 종종 노이즈나 불필요한 정보가 포함되어 있다. 차원 축소를 사용하면 이러한 노이즈나 불필요한 정보를 제거하면서 데이터의 주요 특성만을 추출할 수 있다. 이로 인해 데이터 분석 결과의 정확성이 향상될 수 있다.

## 7. 차원 축소와 머신 러닝
---
### 1) 특성 선택(Feature Selection)과 차원 축소
데이터에서 중요한 정보를 추출하는 방법에는 크게 **특성 선택**과 **차원 축소** 두 가지 방식이 있다
특성 선택은 데이터의 원래 특성 중에서 가장 정보가 많은 특성만을 선택하는 방법이다. 예를 들면, 100개의 특성 중에서 중요한 특성 10개만을 선택하여 사용할 수 있다. 이 방식은 해석이 비교적 쉽고 원래 데이터의 의미를 그대로 유지한다는 장점이 있다.
반면 차원 축소는 기존 특성을 새로운 특성으로 변환하는 것이다. 예를 들어, PCA를 사용하면 100개의 특성이 10개의 주성분으로 변환된다. 변환된 특성은 원래의 특성과는 다른 의미를 가질 수 있지만, 데이터가 가지고 있는 중요한 정보는 최대한 유지하려고 한다.
### 2) 머신 러닝에서의 차원 축소의 영향
차원 축소는 머신 러닝 모델의 성능에 크게 영향을 미치는 요소 중 하나다. 차원 축소의 영향으로는 계산량의 감소와 과적합의 방지가 있다. 특성의 수를 줄임으로써, 모델은 학습을 더 빠르게 수행할 수 있게 된다. 또한, 데이터에 많은 특성이 있는 경우 머신 러닝 모델은 학습 데이터에 과하게 맞춰지는 **과적합**의 위험이 증가하는데, 차원 축소를 통해 불필요한 특성들을 제거함으로써 과적합을 방지할 수 있다.

그러나 차원 축소에도  중요한 정보의 손실과 같은 단점이 있다. 차원 축소의 방법을 잘못 선택하면 기존 데이터에서 중요한 정보가 사라지게 되는데, 이 결과로 머신 러닝 모델의 성능이 저하될 가능성이 있다. 또한, 차원이 축소된 데이터는 기존의 데이터와는 다른 의미를 가지게 될 수 있기 때문에, 그 결과를 해석하는 것이 더 복잡하고 어려워질 수 있다.