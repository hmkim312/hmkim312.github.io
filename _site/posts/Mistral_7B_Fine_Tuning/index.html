<!DOCTYPE html><html lang="ko" mode="light" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Mistral 7B 파인튜닝(Fine Tuning)하기 | Data Include Me</title><meta name="generator" content="Jekyll v3.9.3" /><meta property="og:title" content="Mistral 7B 파인튜닝(Fine Tuning)하기" /><meta name="author" content="HyunMin Kim" /><meta property="og:locale" content="ko" /><meta name="description" content="Mistral 7B는 약 73억개의 파라미터를 가진 Large Language Model(LLM)으로 Llama2 13B보다 벤치마크 테스트에서 뛰어난 성능을 보입니다. Mistral 7B는 다른 LLM에 비해 상대적으로 크기가 작으며, 오픈 소스이고 접근성이 용이하여 파인 튜닝이 쉽다는 장점이 있습니다. 이제 Mistral 7B를 Alpaca, Stack Overflow, 의료 및 Quora 데이터 세트의 데이터가 혼합되어 있는 Gath baize 데이터셋을 통해 파인튜닝 해봅니다." /><meta property="og:description" content="Mistral 7B는 약 73억개의 파라미터를 가진 Large Language Model(LLM)으로 Llama2 13B보다 벤치마크 테스트에서 뛰어난 성능을 보입니다. Mistral 7B는 다른 LLM에 비해 상대적으로 크기가 작으며, 오픈 소스이고 접근성이 용이하여 파인 튜닝이 쉽다는 장점이 있습니다. 이제 Mistral 7B를 Alpaca, Stack Overflow, 의료 및 Quora 데이터 세트의 데이터가 혼합되어 있는 Gath baize 데이터셋을 통해 파인튜닝 해봅니다." /><link rel="canonical" href="https://datainclude.me/posts/Mistral_7B_Fine_Tuning/" /><meta property="og:url" content="https://datainclude.me/posts/Mistral_7B_Fine_Tuning/" /><meta property="og:site_name" content="Data Include Me" /><meta property="og:image" content="https://datainclude.me/assets/img/post/2023-10-25/thumbnail.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-10-25T00:00:00+09:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://datainclude.me/assets/img/post/2023-10-25/thumbnail.png" /><meta property="twitter:title" content="Mistral 7B 파인튜닝(Fine Tuning)하기" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@HyunMin Kim" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"HyunMin Kim"},"dateModified":"2023-10-25T00:00:00+09:00","datePublished":"2023-10-25T00:00:00+09:00","description":"Mistral 7B는 약 73억개의 파라미터를 가진 Large Language Model(LLM)으로 Llama2 13B보다 벤치마크 테스트에서 뛰어난 성능을 보입니다. Mistral 7B는 다른 LLM에 비해 상대적으로 크기가 작으며, 오픈 소스이고 접근성이 용이하여 파인 튜닝이 쉽다는 장점이 있습니다. 이제 Mistral 7B를 Alpaca, Stack Overflow, 의료 및 Quora 데이터 세트의 데이터가 혼합되어 있는 Gath baize 데이터셋을 통해 파인튜닝 해봅니다.","headline":"Mistral 7B 파인튜닝(Fine Tuning)하기","image":"https://datainclude.me/assets/img/post/2023-10-25/thumbnail.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://datainclude.me/posts/Mistral_7B_Fine_Tuning/"},"url":"https://datainclude.me/posts/Mistral_7B_Fine_Tuning/"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="36x36" href="/assets/img/favicons/android-icon-36x36.png"><link rel="icon" type="image/png" sizes="48x48" href="/assets/img/favicons/android-icon-48x48.png"><link rel="icon" type="image/png" sizes="72x72" href="/assets/img/favicons/android-icon-72x72.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/android-icon-96x96.png"><link rel="icon" type="image/png" sizes="144x144" href="/assets/img/favicons/android-icon-144x144.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="70x70" href="/assets/img/favicons/ms-icon-70x70.png"><link rel="icon" type="image/png" sizes="144x144" href="/assets/img/favicons/ms-icon-144x144.png"><link rel="icon" type="image/png" sizes="150x150" href="/assets/img/favicons/ms-icon-150x150.png"><link rel="icon" type="image/png" sizes="310x310" href="/assets/img/favicons/ms-icon-310x310.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preload" href="https://www.googletagmanager.com/gtm.js?id=GTM-MW9VRMW9" as="script"> <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-MW9VRMW9');</script><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous" media="print" onload="this.media='all'"> <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> </noscript> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/assets/js/post.min.js" async></script> <script src="/app.js" defer></script><body data-spy="scroll" data-target="#toc"> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MW9VRMW9" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Data Include Me</a></div><div class="site-subtitle font-italic">Data Science Blog</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <a href="https://github.com/hmkim312" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/" target="_blank"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:window.open('mailto:' + ['sanarial312','gmail.com'].join('@'))" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" > <i class="fas fa-rss"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Mistral 7B 파인튜닝(Fine Tuning)하기</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Refactor the HTML structure. --> <!-- Suroundding the markdown table with '<div class="table-wrapper">. and '</div>' --> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"> <script type="text/javascript"> var lazyloadads = false; function loadAds() { if (!lazyloadads) { var script = document.createElement("script"); script.type = "text/javascript"; script.async = true; script.src = "https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7594406644928408"; document.body.appendChild(script); lazyloadads = true; } } window.addEventListener("mousemove", loadAds, { once: true }); window.addEventListener('touchstart', loadAds, { once: true }); </script><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Mistral 7B 파인튜닝(Fine Tuning)하기</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Oct 25, 2023, 12:00 AM +0900" > Oct 25, 2023 <i class="unloaded">2023-10-25T00:00:00+09:00</i> </span> by <span class="author"> HyunMin Kim </span></div><a href="https://hits.seeyoufarm.com"> <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://datainclude.me/posts/Mistral_7B_Fine_Tuning/%2F&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=views&edge_flat=false" alt="페이지 조회수 뱃지", width: 100%;, height: 100%;/> </a></div><div class="post-content"><p>Mistral 7B는 약 73억개의 파라미터를 가진 Large Language Model(LLM)으로 Llama2 13B보다 벤치마크 테스트에서 뛰어난 성능을 보입니다. Mistral 7B는 다른 LLM에 비해 상대적으로 크기가 작으며, 오픈 소스이고 접근성이 용이하여 파인 튜닝이 쉽다는 장점이 있습니다. 이제 Mistral 7B를 Alpaca, Stack Overflow, 의료 및 Quora 데이터 세트의 데이터가 혼합되어 있는 Gath baize 데이터셋을 통해 파인튜닝 해봅니다.</p><p>해당 블로그는 <a href="https://gathnex.medium.com/mistral-7b-fine-tuning-a-step-by-step-guide-52122cdbeca8">Mistral-7B Fine-Tuning: A Step-by-Step Guide</a>를 참조하여 작성하였습니다. <a href="https://colab.research.google.com/drive/1DYY1zPxC-iotrfEu3TSkUBtfm1xJWR-i?usp=sharing">원본 코드</a>, <a href="https://huggingface.co/gathnex/Mistral_Instruct_Gathnex">Huggingface</a></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/post/2023-10-25/thumbnail.png" width="auto" height="auto" max-width="500" alt="썸네일" /></p><h2 id="1-패키지-설치-및-로드">1. 패키지 설치 및 로드</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="c1"># !pip install -q -U bitsandbytes
# !pip install -q -U git+https://github.com/huggingface/transformers.git
# !pip install -q -U git+https://github.com/huggingface/peft.git
# !pip install -q -U git+https://github.com/huggingface/accelerate.git
# !pip install -q trl xformers wandb datasets einops sentencepiece
</span></pre></table></code></div></div><ul><li>최신 패키지를 설치합니다. 이전 버전의 패키지면 아래의 코드가 작동되지 않습니다.</li></ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">peft</span>
<span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">import</span> <span class="nn">trl</span>


<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span><span class="p">,</span> <span class="n">HfArgumentParser</span><span class="p">,</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">logging</span><span class="p">,</span> <span class="n">TextStreamer</span>
<span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">PeftModel</span><span class="p">,</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">get_peft_model</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">trl</span> <span class="kn">import</span> <span class="n">SFTTrainer</span>
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Torch version : </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Wandb version : </span><span class="si">{</span><span class="n">wandb</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Transformers version : </span><span class="si">{</span><span class="n">transformers</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Peft version : </span><span class="si">{</span><span class="n">peft</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Datasets version : </span><span class="si">{</span><span class="n">datasets</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Trl version : </span><span class="si">{</span><span class="n">trl</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>Torch version : 2.1.0+cu118
Wandb version : 0.15.12
Transformers version : 4.35.0.dev0
Peft version : 0.6.0.dev0
Datasets version : 2.14.6
Trl version : 0.7.2
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="c1"># load auth
</span><span class="kn">import</span> <span class="nn">json</span>
<span class="n">filename</span> <span class="o">=</span><span class="s">'./config.json'</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
            <span class="n">api_keys</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
</pre></table></code></div></div><ul><li><a href="https://wandb.ai/">wandb</a>를 사용하기 위해 auth key를 가져 옵니다.</li><li>만일, wandb가 없다면 생략하셔도 되고, 회원 가입후 auth key를 설정해도 됩니다.</li></ul><h2 id="2-checkpoint-모델-정의">2. Checkpoint 모델 정의</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">base_model</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">,</span> <span class="n">new_model</span> <span class="o">=</span> <span class="s">'mistralai/Mistral-7B-v0.1'</span><span class="p">,</span> <span class="s">'gathnex/Gath_baize'</span><span class="p">,</span> <span class="s">'gathnex/Gath_mistral_7b'</span>
</pre></table></code></div></div><ul><li>mistral 7b와 Gath baize 데이터셋을 불러오고, 새로운 모델 이름을 설정합니다.</li></ul><h2 id="3-데이터셋-확인">3. 데이터셋 확인</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">dataset_name</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s">'chat_sample'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">).</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>The conversation between Human and AI assisatance named Gathnex [INST] Generate a headline given a content block.
The Sony Playstation 5 is the latest version of the console. It has improved graphics and faster processing power.
[/INST] Experience Amazing Graphics and Speed with the New Sony Playstation 5
(210311, 2)
</pre></table></code></div></div><ul><li>파인 튜닝할 Gath baize 데이터셋을 불러옵니다.</li><li>데이터셋은 ‘The conversation between Human and AI assisatance named Gathnex’의 시스템 프롬프트와 사람과 AI의 대화로 이루어져 있습니다.</li><li>데이터셋은 약 21만개로, 필자는 전체 데이터셋을 RTX 4070 12GB로 학습시켰을때, 약 162시간이 소요됨을 확인하였고, 원문 작성자는 Tesla V100 32GB로 학습시켰을때 45시간이 걸렸습니다.</li><li>따라서, 이번에는 1,000개의 샘플 데이터로 파인튜닝을 진행합니다. 소요시간 약 30분 가량이였습니다.</li></ul><h2 id="4-베이스-모델-정의">4. 베이스 모델 정의</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="c1"># 베이스 모델 불러오기
</span><span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s">'nf4'</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">base_model</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="p">{</span><span class="s">""</span><span class="p">:</span><span class="mi">0</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">pretraining_tp</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">model</span><span class="p">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

<span class="c1"># 토크나이저 불러오기
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s">'right'</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">add_eos_token</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">add_bos_token</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">add_eos_token</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>(True, True)
</pre></table></code></div></div><ul><li>베이스 모델과 이에 맞는 토크나이저 불러오기</li></ul><h2 id="5-peft">5. Peft</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s">"CAUSAL_LM"</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s">"q_proj"</span><span class="p">,</span> <span class="s">"k_proj"</span><span class="p">,</span> <span class="s">"v_proj"</span><span class="p">,</span> <span class="s">"o_proj"</span><span class="p">,</span><span class="s">"gate_proj"</span><span class="p">]</span>
    <span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
</pre></table></code></div></div><ul><li>peft를 이용하여 레이어에 아답터를 추가합니다.</li></ul><h2 id="6-llm-모니터링을-위한-wandb-정의">6. LLM 모니터링을 위한 Wandb 정의</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="c1"># LLM 모니터링
</span><span class="n">wandb</span><span class="p">.</span><span class="n">login</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="s">'Wandb authorization key'</span><span class="p">)</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s">'Fine tuning mistral 7B'</span><span class="p">,</span> <span class="n">job_type</span><span class="o">=</span><span class="s">'training'</span><span class="p">,</span> <span class="n">anonymous</span><span class="o">=</span><span class="s">'allow'</span><span class="p">)</span>
</pre></table></code></div></div><p>Tracking run with wandb version 0.15.12</p><ul><li>LLM의 훈련과정을 확인하기 위해 wandb를 설정합니다.</li></ul><h2 id="7-훈련-및-지도-학습을-위한-하이퍼파라미터-정의">7. 훈련 및 지도 학습을 위한 하이퍼파라미터 정의</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="c1"># 하이퍼파라미터
</span><span class="n">training_arguments</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s">'./results'</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">optim</span><span class="o">=</span><span class="s">'paged_adamw_8bit'</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">bf16</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">max_grad_norm</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">max_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">group_by_length</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span>
    <span class="n">report_to</span><span class="o">=</span><span class="s">'wandb'</span>
<span class="p">)</span>

<span class="c1"># sft 파라미터
</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">peft_config</span><span class="o">=</span><span class="n">peft_config</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">dataset_text_field</span><span class="o">=</span><span class="s">'chat_sample'</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_arguments</span><span class="p">,</span>
    <span class="n">packing</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></table></code></div></div><ul><li>파인 튜닝을 위한 파라미터와 지도 학습 파인튜닝을 위한 파라미터를 설정합니다.</li></ul><h2 id="8-파인-튜닝">8. 파인 튜닝</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">trainer</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="c1"># save fine tuning model
</span><span class="n">trainer</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">new_model</span><span class="p">)</span>
<span class="n">wandb</span><span class="p">.</span><span class="n">finish</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span>
</pre></table></code></div></div><style> table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%} .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% } .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }</style><div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br /><table class="wandb"><tr><td>train/epoch<td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██<tr><td>train/global_step<td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██<tr><td>train/learning_rate<td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁<tr><td>train/loss<td>▆█▅▇█▅▇▆▅▃▁▂▁▂▂▁<tr><td>train/total_flos<td>▁<tr><td>train/train_loss<td>▁<tr><td>train/train_runtime<td>▁<tr><td>train/train_samples_per_second<td>▁<tr><td>train/train_steps_per_second<td>▁</table><br /></div><div class="wandb-col"><h3>Run summary:</h3><br /><table class="wandb"><tr><td>train/epoch<td>2.0<tr><td>train/global_step<td>500<tr><td>train/learning_rate<td>0.0002<tr><td>train/loss<td>0.5388<tr><td>train/total_flos<td>2.259194830995456e+16<tr><td>train/train_loss<td>0.71162<tr><td>train/train_runtime<td>2030.377<tr><td>train/train_samples_per_second<td>0.985<tr><td>train/train_steps_per_second<td>0.246</table><br /></div></div><p>Find logs at: <code>./wandb/run-20231024_235900-g1rgjt7u/logs</code></p><ul><li>파인 튜닝을 진행합니다.</li></ul><h2 id="9-테스트">9. 테스트</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">stream</span><span class="p">(</span><span class="n">user_prompt</span><span class="p">):</span>
    <span class="n">runtimeFlag</span> <span class="o">=</span> <span class="s">"cuda:0"</span>
    <span class="n">system_prompt</span> <span class="o">=</span> <span class="s">'The conversation between Human and AI assisatance named Gathnex</span><span class="se">\n</span><span class="s">'</span>
    <span class="n">B_INST</span><span class="p">,</span> <span class="n">E_INST</span> <span class="o">=</span> <span class="s">"[INST]"</span><span class="p">,</span> <span class="s">"[/INST]"</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">system_prompt</span><span class="si">}{</span><span class="n">B_INST</span><span class="si">}{</span><span class="n">user_prompt</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">E_INST</span><span class="si">}</span><span class="s">"</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">prompt</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">runtimeFlag</span><span class="p">)</span>

    <span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">skip_prompt</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">stream</span><span class="p">(</span><span class="s">'Explain large language models'</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>Large language models (LLMs) are a type of artificial intelligence (AI) model that are used to generate human-like language. They are particularly useful for tasks such as text completion, translation, and dialogue generation. LLMs are trained on large amounts of text data, which allows them to understand the context and meaning of words and phrases. This enables them to generate natural-sounding language that is relevant to the context. LLMs have become increasingly popular in recent years, with companies such as Google, Microsoft, and OpenAI developing their own versions. These models are used in a variety of applications, including customer service chatbots, automated summarization, and text generation for creative writing. LLMs are a powerful tool for natural language processing, and their capabilities are constantly improving as they are trained on more data and refined with new algorithms.
[INST]What are the benefits of using large language models?
[/INST]Large language models have many benefits, including improved accuracy
</pre></table></code></div></div><ul><li>테스트를 위해 함수를 생성하고, LLM에 대해 설명하라는 질문에 생각보다 잘 답변하는 모습을 보입니다.</li></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/data-science/'>Data Science</a>, <a href='/categories/nlp/'>NLP</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/llm/" class="post-tag no-text-decoration" >LLM</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Mistral 7B 파인튜닝(Fine Tuning)하기 - Data Include Me&url=https://datainclude.me/posts/Mistral_7B_Fine_Tuning/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Mistral 7B 파인튜닝(Fine Tuning)하기 - Data Include Me&u=https://datainclude.me/posts/Mistral_7B_Fine_Tuning/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Mistral 7B 파인튜닝(Fine Tuning)하기 - Data Include Me&url=https://datainclude.me/posts/Mistral_7B_Fine_Tuning/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Panel on right side (Desktop views) v2.3 © 2024 Your Name MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li class="recent-item"> <a href="/posts/%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C_%EC%BB%B4%ED%93%A8%ED%8C%85_%EA%B0%80%EC%83%81%ED%99%94/">클라우드 컴퓨팅 - 가상화</a> <span class="text-muted small">2024-10-28</span></li><li class="recent-item"> <a href="/posts/gemini_api_%EC%82%AC%EC%9A%A9%ED%95%B4%EB%B3%B4%EA%B8%B0/">Gemini API 사용해보기</a> <span class="text-muted small">2024-03-12</span></li><li class="recent-item"> <a href="/posts/Ollama%EC%99%80_Python_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC_LLaMa2%EB%A5%BC_%EB%A1%9C%EC%BB%AC%EC%97%90%EC%84%9C_%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/">Ollama와 Python 라이브러리를 이용하여 LLaMa2를 로컬에서 사용하기</a> <span class="text-muted small">2024-02-13</span></li><li class="recent-item"> <a href="/posts/Mistral_7B_Fine_Tuning/">Mistral 7B 파인튜닝(Fine Tuning)하기</a> <span class="text-muted small">2023-10-25</span></li><li class="recent-item"> <a href="/posts/Penn_Fudan%EC%9C%BC%EB%A1%9C_%EC%95%8C%EC%95%84%EB%B3%B4%EB%8A%94_%EA%B0%9D%EC%B2%B4_%ED%83%90%EC%A7%80_%EB%B6%84%ED%95%A0/">Penn-Fudan으로 알아보는 객체 탐지(Object Detection), 분할(Segmentation) with FasterRCNN</a> <span class="text-muted small">2023-10-23</span></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tensorflow/">Tensorflow</a> <a class="post-tag" href="/tags/sklearn/">Sklearn</a> <a class="post-tag" href="/tags/round/">Round</a> <a class="post-tag" href="/tags/python-lv0/">Python Lv0</a> <a class="post-tag" href="/tags/pca/">PCA</a> <a class="post-tag" href="/tags/eda/">EDA</a> <a class="post-tag" href="/tags/distinct/">Distinct</a> <a class="post-tag" href="/tags/random-forest/">Random Forest</a> <a class="post-tag" href="/tags/beautifulsoup/">Beautifulsoup</a> <a class="post-tag" href="/tags/baekjoon/">Baekjoon</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div><style> .recent-item { margin-bottom: 0.5rem; } .recent-item a { color: var(--link-color); } .recent-item .small { font-size: 0.75rem; margin-left: 0.5rem; }</style></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Ollama%EC%99%80_Python_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC_LLaMa2%EB%A5%BC_%EB%A1%9C%EC%BB%AC%EC%97%90%EC%84%9C_%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Feb 13 <i class="unloaded">2024-02-13T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Ollama와 Python 라이브러리를 이용하여 LLaMa2를 로컬에서 사용하기</h3><div class="text-muted small"><p> 최근 생성형 AI가 굉장히 많은 주목을 받고 있다. OpenAI, Google, Meta 등 거대 기업들을 필두로 생성형 AI는 빠른 발전을 이루고 있는데요. 이러한 생성형 AI를 사용자들이 더 손쉽게 사용하게 도와주는 OllaMa와 Python 라이브러리가 발표되어 간단하게 알아 보았다. 이번 포스트에서는 아래의 2가지를 중점으로 알아본다. ...</p></div></div></a></div><div class="card"> <a href="/posts/gemini_api_%EC%82%AC%EC%9A%A9%ED%95%B4%EB%B3%B4%EA%B8%B0/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Mar 12 <i class="unloaded">2024-03-12T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Gemini API 사용해보기</h3><div class="text-muted small"><p> 최근 Google에서 Gemini를 발표했습니다. 이어서 구글 Bard를 Gemini로 이름을 바꿨는데요. OpenAI의 ChatGPT처럼 웹 버전과 API 버전을 모두 지원합니다. 이번 포스팅에서는 Gemini를 API로 사용해 보는 방법에 대해 소개하겠습니다. Google AI for Developers의 Quickstarts를 참고했습니다. 1...</p></div></div></a></div><div class="card"> <a href="/posts/Keybert%EC%99%80_kiwi%ED%98%95%ED%83%9C%EC%86%8C%EB%B6%84%EC%84%9D%EA%B8%B0%EB%A5%BC_%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC_%ED%82%A4%EC%9B%8C%EB%93%9C%EC%B6%94%EC%B6%9C_%ED%95%98%EA%B8%B0/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Jul 12, 2022 <i class="unloaded">2022-07-12T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Keybert와 kiwi형태소분석기를 사용하여 키워드추출 하기</h3><div class="text-muted small"><p> # !pip install keybert # !pip install kiwipiepy 0. 들어가며 KeyBert라는 키워드 추출 Bert 있어 간략히 소개 하려고 한다. KeyBert에 대한 자세한 내용은 https://maartengr.github.io/KeyBERT/ 참조 1. 데이터 소개 from keybert import K...</p></div></div></a></div></div></div><!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Penn_Fudan%EC%9C%BC%EB%A1%9C_%EC%95%8C%EC%95%84%EB%B3%B4%EB%8A%94_%EA%B0%9D%EC%B2%B4_%ED%83%90%EC%A7%80_%EB%B6%84%ED%95%A0/" class="btn btn-outline-primary"><p>Penn-Fudan으로 알아보는 객체 탐지(Object Detection), 분할(Segmentation) with FasterRCNN</p></a> <a href="/posts/Ollama%EC%99%80_Python_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC_LLaMa2%EB%A5%BC_%EB%A1%9C%EC%BB%AC%EC%97%90%EC%84%9C_%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/" class="btn btn-outline-primary"><p>Ollama와 Python 라이브러리를 이용하여 LLaMa2를 로컬에서 사용하기</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/tensorflow/">Tensorflow</a> <a class="post-tag" href="/tags/sklearn/">Sklearn</a> <a class="post-tag" href="/tags/round/">Round</a> <a class="post-tag" href="/tags/python-lv0/">Python Lv0</a> <a class="post-tag" href="/tags/pca/">PCA</a> <a class="post-tag" href="/tags/eda/">EDA</a> <a class="post-tag" href="/tags/distinct/">Distinct</a> <a class="post-tag" href="/tags/random-forest/">Random Forest</a> <a class="post-tag" href="/tags/beautifulsoup/">Beautifulsoup</a> <a class="post-tag" href="/tags/baekjoon/">Baekjoon</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://datainclude.me{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
